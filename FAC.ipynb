{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "\n",
    "from transformers import Wav2Vec2Model, AutoProcessor, HubertForCTC\n",
    "from datasets import load_dataset\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "\n",
    "# other imports we might need\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchaudio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torchaudio\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoConfig, Wav2Vec2Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", trust_remote_code=True)\n",
    "dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "\n",
    "ds = load_dataset(\"MushanW/GLOBE_V2\") # GLOBE accented dataset\n",
    "\n",
    "# Load an audio file\n",
    "y, sr = librosa.load('audio.wav')\n",
    "\n",
    "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# GLOBE splits are: test (5.46l), val (4.11k), train (572k)\n",
    "# ds = load_dataset(\"MushanW/GLOBE_V2\") # GLOBE V2 accented dataset\n",
    "\n",
    "split = 'val'\n",
    "sampling_rate = 24000 #24kHz\n",
    "ds = load_dataset(\"MushanW/GLOBE\", split=split, sampling_rate=sampling_rate) # GLOBE accented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use torchaudio to load and librosa to resample\n",
    "# librosa is best for reading .wav but torchaudio is best for reading .mp3\n",
    "# resampling is done with librosa regardless\n",
    "speech, sr = torchaudio.load(path)\n",
    "speech = speech[0].numpy().squeeze()\n",
    "speech = librosa.resample(np.asarray(speech), sr, 16_000)\n",
    "ipd.Audio(data=np.asarray(speech), autoplay=True, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Mel spectrogram\n",
    "S = librosa.feature.melspectrogram(y=array, sr=sampling_rate, n_mels=128, fmax=8000)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.specshow(S_dB, x_axis=\"time\", y_axis=\"mel\", sr=sampling_rate, fmax=8000)\n",
    "plt.colorbar()\n",
    "\n",
    "C = librosa.feature.mfcc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fundamental frequency (f0)\n",
    "y, sr = librosa.load(librosa.ex('trumpet'))\n",
    "f0, voiced_flag, voiced_probs = librosa.pyin(y,sr=sr, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
    "times = librosa.times_like(f0, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "\n",
    "model_name_or_path = \"lighteternal/wav2vec2-large-xlsr-53-greek\"\n",
    "pooling_mode = \"mean\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    label2id={label: i for i, label in enumerate(label_list)},\n",
    "    id2label={i: label for i, label in enumerate(label_list)},\n",
    "    finetuning_task=\"wav2vec2_clf\",\n",
    ")\n",
    "setattr(config, 'pooling_mode', pooling_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set processor and model\n",
    "# processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "# model = HubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "processor = AutoProcessor.from_pretrained('hubert_large_ll60k_finetune_ls960.pt')\n",
    "model = HubertForCTC.from_pretrained('hubert_large_ll60k_finetune_ls960.pt')\n",
    "\n",
    "# audio file is decoded on the fly\n",
    "inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# transcribe speech\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "transcription[0]\n",
    "\n",
    "inputs[\"labels\"] = processor(text=dataset[0][\"text\"], return_tensors=\"pt\").input_ids\n",
    "\n",
    "# compute loss\n",
    "loss = model(**inputs).loss\n",
    "round(loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, HubertForCTC\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained('facebook/hubert-large-ls960-ft')\n",
    "model = HubertForCTC.from_pretrained('facebook/hubert-large-ls960-ft')\n",
    "\n",
    "# audio file is decoded on the fly\n",
    "inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "logits = model(**inputs).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# one-hot predictions\n",
    "num_classes = logits.shape[-1]\n",
    "one_hot = F.one_hot(predicted_ids, num_classes)\n",
    "\n",
    "# transcribe speech\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "\n",
    "\n",
    "# compute loss\n",
    "with processor.as_target_processor():\n",
    "    inputs[\"labels\"] = processor(dataset[0][\"text\"], return_tensors=\"pt\").input_ids\n",
    "\n",
    "loss = model(**inputs).loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pronounciation encoder\n",
    "class PronunciationEncoder(nn.Module):\n",
    "    def __init__(self, one_hots, accent_emb, *args, **kwargs) -> None:\n",
    "        super(PronunciationEncoder, self).__init__()\n",
    "        self.projectionLayer = nn.Linear()\n",
    "        self.transformer = nn.Transformer()\n",
    "        self.dropout = nn.Dropout(p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Wav2Vec2Model\n",
    "\n",
    "# Wav2Vec 2.0 Large (LV-60 + CV + SWBD + FSH)\n",
    "# w2v_large_lv_fsh_swbd_cv_ftls960_updated.pt\n",
    "# model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643-fac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
